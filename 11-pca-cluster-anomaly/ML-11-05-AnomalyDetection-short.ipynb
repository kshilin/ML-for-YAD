{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Обнаружение аномалий в данных </center>\n",
    "\n",
    "<img src='pict/will-myers-986006-unsplash.png'>\n",
    "\n",
    "## <center>  Зачем искать \"аномалии\"? </center> \n",
    "\n",
    "Наиболее часто возникающие задачи:\n",
    "1. Очистка данных тренировочного набора, чтобы на них не переобучится.\n",
    "2. Поиск выбросов в данных, для понимания причин их вызывающих.\n",
    "3. Поиск нового класса данных - как самоцель (как пример новый вид фрода - мошенических действий для получения кредитов, доступа к картам и т.д.)\n",
    "\n",
    "В документации scikit-learn есть [определение](https://scikit-learn.org/stable/modules/outlier_detection.html), которое сводит задачу поиска анаомали к двум классам задач:  \n",
    "\n",
    "- **Outlier detection** (поиск выбросов): тренировочный набор содержит данные, которые определяются как выбросы - данные удаленные от  остальных. Таким образом, алгоритм обнаружения выбросов пытаются найти область, где тренировочный набор наиболее сконцентрирован игнорируя выбросы.\n",
    "\n",
    "- **Novelty detection** (поиск новизны): в тренировочном наборе отсутствуют выбросы, и модель пытается определить, является ли новое наблюдение выбросом. \n",
    "\n",
    "\n",
    "Поэтому **поиск аномалий** может быть как препроцессингом (этапом подготовки и очистки данных), так и целью анализа данных с построением модели по их поиску.\n",
    "\n",
    "\n",
    "## <center>Проблемы поиска аномалий</center>\n",
    "\n",
    "Аномалии на реальных данных не размечены. Поэтому большинство подобных задач сводитсся к алгоритмам обучения без учителя.\n",
    "\n",
    "В связи с этим возникает проблема валидации нашего решения. Так как у нас данные не размечены, то мы не можем достоверно понять как \"отработал\" наш алгоритм. Здесь может помочь \"здавый смысл\" и мнение экспертов в предметной области (не бойтесь задавать вопросы окружающим  вас специалистам). Это поможет в настройке и подгонке параметров вашей модели.\n",
    "\n",
    "В случае решения задачи препроцессинга необходимо помнить, что мы можем \"чистить данные\" только в тренировочном наборе, ни в коем случае не затрагивая тестовые и поверочные наборы!\n",
    "\n",
    "Помним, что \"чистые данные\" спасают от переобучения :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Методы\n",
    "\n",
    "**Базовые статистические (ТВиМС):**\n",
    "- \"на глаз\": визуально по гистограммам (только хвосты, так как гистограммы для дискретных данных - зло, помните контрольную про олимпиады и \"кошкины ушки\" на гистограмме), scaterplot, boxplot, violinplot ...\n",
    "- стандартые статистические характеристики - стандартное отклонение, квантили и процентили  ...\n",
    "- статистические тесты\n",
    "- винзоризинг данных ([Winsorizing](https://en.wikipedia.org/wiki/Winsorizing)) и похожие приемы\n",
    "\n",
    "**Машинное обучение (основное):**\n",
    "1. методы основанные на измерение растояний (евклидовых, манхэтенских и т.д.)(eng: Distance-based): находим центр кластера и считам растояние до его точек, если расстояние больше некоторого порогового значения - аномалия \n",
    "2. методы основанные на плотности данных (eng.: Density-based): находим плотность данных, если плотность ниже порога - аномалия (яркиц пример - DBSCAN)\n",
    "3. методы основанные на моделях (eng.: Model-based): строим модели - \"[Убивайте всех! Господь отличит своих](https://ru.wikipedia.org/wiki/Арнольд_Амальрик)\"\n",
    "    - **машины опорных векторов**: строим линейную разделяющую гиперплоскость между выбросами и данными ([One-Class SVM](http://rvlasveld.github.io/blog/2013/07/12/introduction-to-one-class-support-vector-machines/))\n",
    "    - **изолированный лес**: строим дерево стандартными методами, для расчета количества шагов для достижения каждой точки, мало шагов аномалия, много шагов \"полезные\" данные ([Isolation Forest](https://quantdare.com/isolation-forest-algorithm/)) \n",
    "    - **расширенный изолированный лес**: все тоже, но любые прямые, а только вертикальные и горизонтальные разделяющие [Extented Isolation Forest](https://github.com/sahandha/eif)\n",
    "    - **глубокое обучение** (не в нашем курсе): если кратко, ищем данные, например с самой большой ошибкой восстановления (representation error).\n",
    "    \n",
    "Для True-DатаSатанистов есть [PyOD](https://pyod.readthedocs.io/en/latest/)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv'\n",
    "# уберем последний столбец, так как это целевая функция\n",
    "data = pd.read_csv(path, sep=';').iloc[:,:-1]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Визуальный анализ </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist(figsize=(15, 15));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_nums = 3  # how many plots per row\n",
    "row_nums = math.ceil(len(data.columns) / col_nums)  # how many rows of plots\n",
    "# print(col_nums, row_nums)\n",
    "_, axes = plt.subplots(nrows=row_nums, ncols=col_nums, figsize=(15,15))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    sns.boxplot(data = data.iloc[:,i], ax = ax).set_title(data.columns[i]);\n",
    "    if i+1 == len(data.columns) : break\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_nums = 3  # how many plots per row\n",
    "row_nums = math.ceil(len(data.columns) / col_nums)  # how many rows of plots\n",
    "# print(col_nums, row_nums)\n",
    "fig, axes = plt.subplots(nrows=row_nums, ncols=col_nums, figsize=(15,15))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    sns.violinplot(data = data.iloc[:,i], ax = ax).set_title(data.columns[i])\n",
    "    if i + 1 == len(data.columns) : break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что в данном случае, для анализа корреляционной матрицы удачно использовать **симетричные** цветовые схемы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "corr_matrix = data.corr()\n",
    "mask = np.tril(corr_matrix, k = -1).astype(bool)\n",
    "display(corr_matrix.where(mask))\n",
    "\n",
    "mymap = sns.diverging_palette(240, 240, as_cmap = True)\n",
    "\n",
    "# я не использую mask как опцию в heatmap т.к. она перевернет данные :(\n",
    "sns.heatmap(corr_matrix.where(mask), vmin = -1, vmax = 1,\n",
    "            annot=True, fmt='.2f', center=0, cmap=mymap);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ВНИМАНИЕ !!!! ВАЖНО ПОМНИТЬ матрица симетрична !!! Нужно искать максимумв имин на ТРЕУГОЛЬНОЙ МАТРИЦЕ !!!\n",
    "\n",
    "mask_high_corr = corr_matrix.where(mask).apply(lambda x: abs(x)>0.6).any()\n",
    "columns_high_corr = mask_high_corr[mask_high_corr].index\n",
    "columns_high_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data[columns_high_corr]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features = data.drop(columns = columns_high_corr)\n",
    "data_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data_features);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### считается дольше, но вид более корректный!!!\n",
    "sns.pairplot(data_features, diag_kind=\"kde\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверим теперь корреляционную матрицу\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "corr_matrix = data_features.corr()\n",
    "mask = np.tril(corr_matrix, k=-1).astype(bool)\n",
    "mymap = sns.diverging_palette(150, 150, as_cmap = True)\n",
    "sns.heatmap(corr_matrix.where(mask), vmin = -1, vmax = 1,\n",
    "            annot=True, fmt='.2f', center=0, cmap=mymap);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Статистические методы </center>\n",
    "\n",
    "## <center> 3-сигмы</center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_std(df, column, multiplier=3):\n",
    "    mean = df[column].mean()\n",
    "    std = df[column].std()\n",
    "    lower_bound = mean - multiplier * std\n",
    "    upper_bound = mean + multiplier * std\n",
    "    outliers = ~df[column].between(lower_bound,upper_bound)\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "\n",
    "def anomalies_detection (df, columns=None, function=outlier_std, multiplier = 3):\n",
    "    if columns:\n",
    "        columns_check = columns\n",
    "    else:\n",
    "        columns_check = df.columns\n",
    "    anomalies_columns_table = {}\n",
    "    anomalies = pd.Series(False, index = df.index, name='anomalies')\n",
    "    for column in columns_check:\n",
    "        outliers, lower_bound, upper_bound = function(df, column, multiplier = multiplier)\n",
    "        anomalies[outliers[outliers].index] = True\n",
    "        ### True, False для расчета процентов вычисляем среднее :)\n",
    "        anomalies_columns_table[column] = [upper_bound, lower_bound, sum(outliers), 100*np.mean(outliers)]\n",
    "    anomalies_columns_table = pd.DataFrame(anomalies_columns_table).T\n",
    "    anomalies_columns_table.columns=['upper_bound', 'lower_bound', 'anomalies_count', 'anomalies_pct']\n",
    "    return anomalies_columns_table, anomalies\n",
    "\n",
    "def anomalies_report(anomalies):\n",
    "    return print(\"Аномалий: {}\\nПроцент аномалий:   {:.2f}%\".format(sum(anomalies), 100*np.mean(anomalies))) \n",
    "\n",
    "std_data = data_features.copy()\n",
    "anomalies_columns_table, anomalies = anomalies_detection(std_data, multiplier = 3)    \n",
    "std_data['is_anomalies'] = anomalies\n",
    "\n",
    "\n",
    "display(anomalies_columns_table)\n",
    "anomalies_report(anomalies)\n",
    "display(std_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data = std_data, \n",
    "             hue='is_anomalies', hue_order=[1, 0], \n",
    "             markers=['o','X'],  palette='bright');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data = std_data, \n",
    "             hue='is_anomalies', hue_order=[1, 0], \n",
    "             diag_kind=\"kde\",\n",
    "             diag_kws=dict(common_norm=False), # этот параметр отменяет общую нормировку площади =1\n",
    "             markers=['o','X'],  palette='bright');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data[columns_high_corr]);\n",
    "df = data[columns_high_corr].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_columns_table, anomalies = anomalies_detection(df, multiplier = 3)    \n",
    "df['is_anomalies'] = anomalies\n",
    "\n",
    "display(anomalies_columns_table)\n",
    "anomalies_report(anomalies)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df,\n",
    "             hue='is_anomalies', hue_order=[True, False], \n",
    "             diag_kind=\"kde\",\n",
    "             diag_kws=dict(common_norm=False),\n",
    "             markers=['o','X'],  palette='bright');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Кто сможет сделать график на оси Полный для True и False,  \n",
    "## и второй для True получит дополнительно 10 баллов!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Межквантильное отклонение</center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### необходимо построить собственную функцию основанную на межквантильном отклонении 1Q-IQR и 3Q+IQR, \n",
    "### гдe IQR = 3Q-1Q, Q - квантиль\n",
    "### обязательно использовать функцию anomalies_detection!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Метод на основе измерений расстояний</center>\n",
    "\n",
    "## <center> Метод расчета расстояния от центра кластера </center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "\n",
    "class AnomaliesDistance(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Модель обнаружения индексов на основе вычисления расстояния от центроида кластера\n",
    "    \n",
    "    Метод Fit вычисляет центроид обучающей выборки и используя метод metric в scipy.spatial.distance.cdist, \n",
    "    находит расстояния от центроида до точек обучающей выбороки.\n",
    "    Метод позволяет вычислить \"порог\" на основе заданного \"перцентиля\" для расстояний.\n",
    "    \n",
    "    Метод Predict использует вычесленный \"порог\", чтобы определить является ли новая точка выбросом.\n",
    "    \n",
    "    -----------\n",
    "    Параметры:\n",
    "    - metric: string, default - 'euclidean'\n",
    "        используется для задания метода расчета рассояния (scipy.spatial.distance.cdist)\n",
    "        \n",
    "    - percentile: float in range [0, 100], default - 90\n",
    "        параметр устанавливающий порог принятия решения об аномальности значения\n",
    "    \"\"\"\n",
    "    def __init__(self, metric='euclidean', percentile=90):\n",
    "        self.metric = metric\n",
    "        self.percentile = percentile\n",
    "        \n",
    "    def fit(self, X):\n",
    "        self.centroid = np.mean(X, axis=0).values.reshape(-1, 1).T\n",
    "        distances_train = cdist(self.centroid, X, metric=self.metric).reshape(-1)\n",
    "        self.threshold = np.percentile(distances_train, self.percentile)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        distances = cdist(self.centroid, X, metric=self.metric).reshape(-1)\n",
    "        predictions = (distances > self.threshold).astype(bool)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для расчетов расстояний НЕОБХОДИМО, что бы значения были сопоставивы по осям. Лучше всего использлвать RobustScaler, иначе выбросы повлияют на средее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "scaled_data = pd.DataFrame(\n",
    "    data=scaler.fit_transform(data_features), \n",
    "    columns=data_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidian_model = AnomaliesDistance(metric='euclidean', percentile=90)\n",
    "euclidian_model.fit(scaled_data)\n",
    "euclidian_anomalies = euclidian_model.predict(scaled_data)\n",
    "euclidian_anomalies\n",
    "anomalies_report(euclidian_anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_data = data_features.copy()\n",
    "cluster_data['is_anomalies'] = euclidian_anomalies\n",
    "\n",
    "sns.pairplot(data=cluster_data, \n",
    "             hue='is_anomalies', hue_order=[True, False], \n",
    "             diag_kind=\"kde\",\n",
    "             diag_kws=dict(common_norm=False), \n",
    "             markers=['o','X'],  palette='bright');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отобразим n-мерное пространство на двухмерную плоскость,  а затем попробуем отразить наши аномалии на проекции "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(perplexity = 99, n_jobs = -1) # менял perplexity для получения большей 'красоты'\n",
    "tsne_transformed = tsne.fit_transform(scaled_data)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.scatterplot(x = tsne_transformed[:, 0], y = tsne_transformed[:, 1], hue = euclidian_anomalies);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Метод основанный на плотности данных - DBSCAN </center>\n",
    "\n",
    "## <center> Метод на основе DBSCAN </center>\n",
    "\n",
    "<img src=\"pict/dbscan.png\" width=500>\n",
    "\n",
    "**Визуализация**   https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/\n",
    "\n",
    "**Алгоритм:**\n",
    "1. Выбираем случайную точку и находим её соседей в заданной окрестности (<code>eps</code> - радиус, <code>min_samplesint</code> - \"количество точек\" внутри радиуса, наше пороговое значение)\n",
    "2. Если соседей меньше порогового значения – считаем точку шумом\n",
    "3. Если соседей больше или равно пороговому значению  – объединяем в «плотный» кластер и повторяем поиск соседей\n",
    "4. Если все плотные точки пройдены и помечены как посещенные – выбираем новую не посещенную точку и начинаем сначала\n",
    "5. Повторяем, пока все точки не будут посещены\n",
    "\n",
    "\n",
    "**Преимущества:**\n",
    "- Density-based (плотностной/вероятностный) метод – умеет находить сложные формы кластеров\n",
    "- Поиск выбросов и аномалий в данных за счет точек шума\n",
    "\n",
    "**Недостатки:**\n",
    "- Очень чувствителен к параметру <code>eps</code>\n",
    "\n",
    "**Идея поиска аномалий:** аномалии должны сильно отличаться от основных данных и скорее всего попадут в \"шум\". Поэтому будем увеличивать параметр  <code>eps</code> до тех пор, пока все \"плотные\" данные не окажутся в кластерах, а \"шума\" будет столько, сколько мы предпологам получит аномалий в данных. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# для очень маленький эпсилон и начинаем увеличивать\n",
    "eps = 0.05\n",
    "anomalies_percentage = 1.\n",
    "\n",
    "eps_history = []\n",
    "num_clusters = []\n",
    "anomaly_percentage = []\n",
    "\n",
    "# берем маленький эпсилон и начинаем увеличивать\n",
    "eps = 0.05\n",
    "eps_history = [eps]\n",
    "while anomalies_percentage > 0.1:    \n",
    "    model = DBSCAN(eps=eps).fit(scaled_data)\n",
    "    num_clusters.append(np.unique(model.labels_).size-1)\n",
    "    anomalies = np.where(model.labels_< 0, True, False)\n",
    "\n",
    "    anomalies_percentage = np.mean(anomalies)    \n",
    "    eps_history.append(eps)\n",
    "    anomaly_percentage.append(anomalies_percentage)\n",
    "    eps += 0.05 # внимание важно где разместить! Почему?\n",
    "    \n",
    "model = DBSCAN(eps_history[-1]) # Почему eps_history, а не eps?\n",
    "model.fit(scaled_data)\n",
    "anomalies = np.where(model.labels_< 0, True, False)\n",
    "anomalies_report(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = eps_history[:-1]\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax1.set_ylabel('anomaly percentage', color=color) \n",
    "ax1.plot(iterations, anomaly_percentage, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "\n",
    "ax2 = ax1.twinx()  \n",
    "color = 'tab:red'\n",
    "ax2.set_xlabel('epsilon')\n",
    "ax2.set_ylabel('number of clusters', color=color)\n",
    "ax2.plot(iterations, num_clusters, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "#fig.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_data = data_features.copy()\n",
    "dbscan_data['is_anomalies'] = anomalies\n",
    "\n",
    "sns.pairplot(data=dbscan_data, \n",
    "             hue='is_anomalies', hue_order=[1, 0], \n",
    "             diag_kind=\"kde\",\n",
    "             diag_kws=dict(common_norm=False), \n",
    "             markers=['o','X'],  palette='bright');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "sns.scatterplot(x = tsne_transformed[:, 0], y = tsne_transformed[:, 1], hue = dbscan_data['is_anomalies']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Методы основанные на моделях машинного обучения</center>\n",
    "\n",
    "## <center>One-Class SVM</center>\n",
    "\n",
    "![](https://sandipanweb.files.wordpress.com/2018/04/svm_slack.png?w=676)\n",
    "\n",
    "**Основная идея алгоритма SVM (в случае с классификацией)** - разделить классы гиперплоскостью так, чтобы максимизировать расстояние между ними. Для нелинейного разделения данных используется так называемый Kernel Trick. Это функция, которая способна преобразовать пространство признаков (в том числе нелинейно), без непосредственного преобразования признаков. Крайне эффективна в плане вычисления и потенциально позволяет получать бесконечноразмерные признаковые пространства. \n",
    "\n",
    "Смысл преобразований в том,  что классы, линейно неразделимые в текущем признаковом пространстве, могут стать разделимыми в пространствах более высокой размерности.\n",
    "\n",
    "[One Class SVM](http://rvlasveld.github.io/blog/2013/07/12/introduction-to-one-class-support-vector-machines/) - это одна из форм классического алгоритма SVM, модификация которого заключается в следующей идее:<br> \n",
    "преобразовать признаковое пространство и провести разделяющую гиперплоскость так, чтобы наблюдения лежали как можно дальше от начала координат. <br>\n",
    "В результате получим границу, по одну сторону которой лежат максимально \"плотные\" и похожие друг на друга наблюдения из нашей выборки, а по другую будут находится аномальные значения, не похожие на все остальные.<br>\n",
    "Процент таких аномальных наблюдений, которые модель будет пытаться отделить от основной части выборки, мы снова задаём в самом начале обучения при помощи параметра <code>nu</code>:\n",
    "\n",
    "![](pict/2-Figure1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "one_class_svm = OneClassSVM(nu=0.1, gamma='auto')\n",
    "one_class_svm.fit(scaled_data)\n",
    "svm_outliers = one_class_svm.predict(scaled_data)\n",
    "anomalies = np.where(svm_outliers < 0, True, False)\n",
    "anomalies_report(anomalies)\n",
    "\n",
    "svm_data = data_features.copy()\n",
    "svm_data['is_anomalies'] = np.where(anomalies, True, False)\n",
    "\n",
    "sns.pairplot(data = svm_data, \n",
    "             hue ='is_anomalies', hue_order=[1, 0], \n",
    "             diag_kind=\"kde\",\n",
    "             diag_kws=dict(common_norm=False), \n",
    "             markers=['o','X'],  palette='bright');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "sns.scatterplot(x = tsne_transformed[:, 0], y = tsne_transformed[:, 1], hue = svm_data['is_anomalies']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Преимущества:**\n",
    "- благодаря kernel trick, модель способна проводить **нелинейные разделяющие границы**;\n",
    "- особенно удобно использовать, когда в данных недостаточно \"плохих\" наблюдений, чтобы использовать стандартный подход обучения с учителем - бинарную классификацию.\n",
    "\n",
    "**Недостатки:**\n",
    "- может очень сильно **переобучиться** и выдавать большое количество ложно отрицательных результатов, если разделяющий зазор слишком мал;\n",
    "- сложность точной настройки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Isolation Forest </center>\n",
    "\n",
    "**Основная идея алгоритма Isolation Forest** заключатся в  вычислении шагов \"изолирования\" каждой точки данных.  Если точка изолируются \"слишком легко\", то скорее всего она лежит достаточно далего от остальных точек и является выбросом. Если \"слишком тяжело\" - скорее всего она лежит среди остальных точек и выбросом не является. \n",
    "\n",
    "<img src=\"https://quantdare.com/wp-content/uploads/2018/03/outlier2.gif\" width=600>\n",
    "\n",
    "<img src=\"https://quantdare.com/wp-content/uploads/2018/03/inlier2.gif\" width=600>\n",
    "\n",
    "С математической точки зрения разбиение всех точек данных может быть представлено древовидной структурой, в то время как количество шагов, необходимых для изоляции точки, можно интерпретировать как длину пути внутри дерева данного дерева, начиная от корня к вершине. \n",
    "\n",
    "Например, длина подобного пути на нижней схеме больше, чем длина пути на верхней схеме. \n",
    "\n",
    "Подробнее  [тут ...](https://quantdare.com/isolation-forest-algorithm/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "isolation_forest = IsolationForest(n_estimators=100, \n",
    "                                   contamination=0.1, \n",
    "                                   #max_features=1, \n",
    "                                   #bootstrap=True\n",
    "                                  )\n",
    "isolation_forest.fit(scaled_data.values)\n",
    "\n",
    "isolation_predict = isolation_forest.predict(scaled_data.values)\n",
    "anomalies = np.where(isolation_predict < 0, True, False)\n",
    "anomalies_report(anomalies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isolation_data = data_features.copy()\n",
    "isolation_data['is_anomalies'] = anomalies\n",
    "\n",
    "sns.pairplot(data=isolation_data,\n",
    "             hue ='is_anomalies', hue_order=[1, 0], \n",
    "             diag_kind=\"kde\",\n",
    "             diag_kws=dict(common_norm=False), \n",
    "             markers=['o','X'],  palette='bright');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Преимущества:**\n",
    "- данные не нужно приводить к сопостовимым разменростям, как и любые леса;\n",
    "- быстрый и легко параллелится.\n",
    "\n",
    "**Недостатки:**\n",
    "- прямоугольность (поясним ниже);\n",
    "- группы аномальных точек могут объеденится в кластер и \"маскироваться\" под данные;\n",
    "- как для любого леса, может быть сложно подобрать параметры не имея хотя бы минимальной разметки или предположения о количестве аномалий."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Extended Isolation Forest </center>\n",
    "\n",
    "**Основная идея алгоритма Extented Isolation Forest** заключатся в  вычислении шагов \"изолирования\" каждой точки данных, но в отличии от обычного леса для разбиения используются прямые не перпендикулярные осям ординат.  Если точка изолируются \"слишком легко\", то скорее всего она лежит достаточно далего от остальных точек и является выбросом. Если \"слишком тяжело\" - скорее всего она лежит среди остальных точек и выбросом не является. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зачем так сложно городить \"огород\"?\n",
    "\n",
    "Исходные данные:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/sahandha/eif/master/figures/Training.png\" width=600>\n",
    "\n",
    "Isolation Forest (посмотрите на прямоугольность результатов):\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/sahandha/eif/master/figures/scores_maps.png\" width=600>\n",
    "\n",
    "Extented Isolation Forest:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/sahandha/eif/master/figures/scores_maps_extended.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как это работает?<br>\n",
    "Разбиение осуществляется прямыми (или плоскостями) не являющихся нормальными к ортам.\n",
    "<img src=\"https://csdl-images.ieeecomputer.org/trans/tk/2021/04/figures/harir8-2947676.gif\" width=600>\n",
    "\n",
    "В результате нет видимых артефактов на ппересечениях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важно! [тут оригинал материала](https://github.com/sahandha/eif?tab=readme-ov-file), но появились проблемы с `Cython`  \n",
    "Поэтому [будем брать тут](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/eif.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Самостоятельно,** сделайте \"зашумленный синус\" и сравните работу двух лесов. В чем принципиальная разница?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import eif\n",
    "\n",
    "# eif_forest = eif.iForest(scaled_data.values, ntrees=200, sample_size=256, ExtensionLevel=1)\n",
    "# eif_score = eif_forest.compute_paths(X_in=scaled_data.values) \n",
    "# bound = np.percentile(eif_score, 90)\n",
    "# anomalies = np.where(eif_score > bound, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## СТАВИТЬ только отсюда!\n",
    "# https://docs.h2o.ai/prior_h2o/index.html\n",
    "# pip install http://h2o-release.s3.amazonaws.com/h2o/rel-3.46.0/7/Python/h2o-3.46.0.7-py2.py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.estimators import H2OExtendedIsolationForestEstimator\n",
    "h2o.init();\n",
    "\n",
    "eif_forest  = H2OExtendedIsolationForestEstimator(model_id = \"eif.hex\",\n",
    "                                          ntrees = 200,\n",
    "                                          sample_size = 256,\n",
    "                                          extension_level = #1\n",
    "                                                  scaled_data.columns.shape[0] - 1\n",
    "                                                 )\n",
    "eif_forest.train(\n",
    "          x = scaled_data.columns.to_list(),\n",
    "          training_frame = h2o.H2OFrame(scaled_data)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eif_result = eif_forest.predict(h2o.H2OFrame(scaled_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install conda-forge::polars для увеличения скорости\n",
    "eif_score = eif_result[\"anomaly_score\"].as_data_frame(use_multi_thread=True)\n",
    "anomalies = np.where(eif_score > 0.7,  True, False)\n",
    "anomalies.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_report(anomalies)\n",
    "\n",
    "eif_data = data_features.copy()\n",
    "eif_data['is_anomalies'] = anomalies\n",
    "\n",
    "sns.pairplot(data = eif_data,\n",
    "             hue ='is_anomalies', hue_order=[1, 0], \n",
    "             diag_kind=\"kde\",\n",
    "             diag_kws=dict(common_norm=False), \n",
    "             markers=['o','X'],  palette='bright');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "sns.scatterplot(x = tsne_transformed[:, 0], y = tsne_transformed[:, 1], hue = eif_data['is_anomalies']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bound = np.percentile(eif_score, 90)\n",
    "anomalies = np.where(eif_score > bound, True, False)\n",
    "\n",
    "anomalies_report(anomalies)\n",
    "\n",
    "eif_data = data_features.copy()\n",
    "eif_data['is_anomalies'] = anomalies\n",
    "\n",
    "sns.pairplot(data = eif_data,\n",
    "             hue ='is_anomalies', hue_order=[1, 0], \n",
    "             diag_kind=\"kde\",\n",
    "             diag_kws=dict(common_norm=False), \n",
    "             markers=['o','X'],  palette='bright');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "sns.scatterplot(x = tsne_transformed[:, 0], y = tsne_transformed[:, 1], hue = eif_data['is_anomalies']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Финальное сравнение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравнение лесов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinat(x):\n",
    "    x = list(x)\n",
    "    if x == [True, True]:\n",
    "       return 'True_all'\n",
    "    elif x == [False, False]:\n",
    "       return 'False_all'\n",
    "    elif x == [True, False]:\n",
    "       return 'Left'\n",
    "    elif x == [False, True]:\n",
    "       return 'Rigth'\n",
    "\n",
    "\n",
    "left = isolation_data['is_anomalies']\n",
    "right = eif_data['is_anomalies']\n",
    "\n",
    "anomalies = pd.concat([left,right],axis = 1).apply(lambda x: combinat(x.values), axis =1)\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.scatterplot(x = tsne_transformed[:, 0], y = tsne_transformed[:, 1], hue = anomalies);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общее лесов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary  = pd.concat([std_data['is_anomalies'],\n",
    "                     cluster_data['is_anomalies'],\n",
    "                     dbscan_data['is_anomalies'],\n",
    "                     svm_data['is_anomalies'],\n",
    "                     isolation_data['is_anomalies'], \n",
    "                     eif_data['is_anomalies']],axis = 1)\n",
    "summary.columns = ['std', 'cluster','dbscan', 'svm', 'iso', 'eif']\n",
    "summary['sum'] =  summary.mean(axis=1)\n",
    "\n",
    "\n",
    "plt.hist(summary['sum'], alpha=0.6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_result = data_features.copy()\n",
    "summary_result['is_anomalies'] = np.where(summary['sum']>=0.8,True,False)\n",
    "summary_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "sns.scatterplot(x = tsne_transformed[:, 0], y = tsne_transformed[:, 1], hue = summary_result['is_anomalies']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary['sum'] =  summary[['dbscan', 'svm', 'iso', 'eif']].mean(axis=1)\n",
    "plt.hist(summary['sum'], alpha=0.6);\n",
    "summary_result['is_anomalies'] = np.where(summary['sum']>=0.9,True,False);\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.scatterplot(x = tsne_transformed[:, 0], y = tsne_transformed[:, 1], hue = summary_result['is_anomalies']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data = summary_result,\n",
    "             hue ='is_anomalies', hue_order=[True, False], \n",
    "             diag_kind=\"kde\",\n",
    "             diag_kws=dict(common_norm=False), \n",
    "             markers=['o','X'],  palette='bright');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Что еще почитать / посмотреть:**\n",
    "\n",
    "Библиотека по поиску аномалий https://pyod.readthedocs.io/en/latest/\n",
    "\n",
    "Сравнение методов `scikit-learn`  \n",
    "https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_anomaly_comparison.html#sphx-glr-auto-examples-miscellaneous-plot-anomaly-comparison-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
